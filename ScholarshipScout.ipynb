{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p7jqhAsWef5"
      },
      "source": [
        "# ScholarshipScout\n",
        "\n",
        "This notebook implements the ScholarshipScout project, an Multi-Agent AI based web scraper to search for fully funded doctorate positions from popular academic position websites.\n",
        "\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "First, let's install the required packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFlsf4kOWhLf"
      },
      "outputs": [],
      "source": [
        "!pip install rich http3 httpx pandas docopt openpyxl brotlipy bs4 urllib3\n",
        "!pip install --upgrade httpx httpcore\n",
        "!pip install nest_asyncio\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhX5AA0eWiKq"
      },
      "source": [
        "## Step 2: Import Libraries and Define Constants\n",
        "\n",
        "Now, let's import the necessary libraries and define some constants.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSJfccqRWjJs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import sys\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "import rich\n",
        "from rich.console import Console\n",
        "from datetime import date\n",
        "from dataclasses import dataclass\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from pathlib import Path\n",
        "import json\n",
        "import httpx\n",
        "\n",
        "console = Console()\n",
        "\n",
        "__version__ = \"0.4.7\"\n",
        "import urllib3\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBmIL6QCWk-v"
      },
      "source": [
        "## Step 3: Define Config Class\n",
        "\n",
        "This class holds configuration details for different repositories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftINrr-0Wl_d"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    config = {\n",
        "        'scholarshipdb': {\n",
        "            'sought#': 'h1.title',\n",
        "            'query': 'https://scholarshipdb.net/scholarships/Program-PhD?page={page}&q={fields}',\n",
        "            'title': 'h4 a',\n",
        "            'country': '.list-unstyled a.text-success',\n",
        "            'date': '.list-unstyled span.text-muted',\n",
        "            'link': \".list-unstyled h4 a\",\n",
        "        },\n",
        "        'findaphd': {\n",
        "            'sought#': 'h4.course-count.d-none.d-md-block.h6.mb-0.mt-1',\n",
        "            'query': 'https://www.findaphd.com/phds/non-eu-students/?01w0&Keywords={fields}&PG={page}',\n",
        "            'title': \"h4 text-dark mx-0 mb-3\",\n",
        "            'country': \"country-flag img-responsive phd-result__dept-inst--country-icon\",\n",
        "            'date': \"apply py-2 small\",\n",
        "            'link': \"h4 text-dark mx-0 mb-3\",\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(self, repo='scholarshipdb'):\n",
        "        self.repo = repo\n",
        "\n",
        "    @classmethod\n",
        "    def repos(cls):\n",
        "        return ','.join(list(cls.config))\n",
        "\n",
        "    @property\n",
        "    def query(self):\n",
        "        return Config.config[self.repo]['query']\n",
        "\n",
        "    @property\n",
        "    def sought(self):\n",
        "        return Config.config[self.repo]['sought#']\n",
        "\n",
        "    @property\n",
        "    def title(self):\n",
        "        return Config.config[self.repo]['title']\n",
        "\n",
        "    @property\n",
        "    def link(self):\n",
        "        return Config.config[self.repo]['link']\n",
        "\n",
        "    @property\n",
        "    def country(self):\n",
        "        return Config.config[self.repo]['country']\n",
        "\n",
        "    @property\n",
        "    def date(self):\n",
        "        return Config.config[self.repo]['date']\n",
        "\n",
        "    @property\n",
        "    def baseURL(self):\n",
        "        return next(re.finditer(r'^.+?[^\\/:](?=[?\\/]|$)', Config.config[self.repo]['query'])).group()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTI-U4u1Wm1m"
      },
      "source": [
        "## Step 4: Define PhDSeeker Class\n",
        "\n",
        "This is the main class that handles the scraping and data processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_ILMgjQWp89"
      },
      "outputs": [],
      "source": [
        "class PhDSeeker:\n",
        "    def __init__(self, keywords: str, repos: str = 'scholarshipdb, findaphd', maxpage: int = 10, desired_countries: str = None):\n",
        "        self.repos = list(map(str.strip, repos.split(',')))\n",
        "        self.keywords = keywords\n",
        "        self.fields = '%20'.join([f\"\\\"{item.replace(' ', '%20')}\\\"\" for item in map(str.strip, keywords.split(','))])\n",
        "        self.desired_countries = [country.strip().lower() for country in desired_countries.split(\",\")] if desired_countries else []\n",
        "        self.titles = []\n",
        "        self.countries = []\n",
        "        self.dates = []\n",
        "        self.links = []\n",
        "        self.maxpage = maxpage\n",
        "        self.file_name = f\"PhD_Positions_{date.today()}[{keywords}]\"\n",
        "        self.df = None\n",
        "        self.sought_number = 0\n",
        "        self.found_number = 0\n",
        "\n",
        "    async def __get_page__(self, repo, page):\n",
        "        headers = {\n",
        "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
        "        }\n",
        "        c = Config(repo)\n",
        "        try:\n",
        "            query = c.query.format(fields=self.fields, page=page)\n",
        "            async with httpx.AsyncClient() as client:\n",
        "                response = await client.get(query, headers=headers, follow_redirects=True)\n",
        "            soup = bs(response.text, \"html.parser\")\n",
        "            if page == 1:\n",
        "                if (n := soup.select_one(c.sought)) is not None:\n",
        "                    foundPositions = int(re.search('(\\d+[,\\d*]*)', n.text)[1].replace(',',''))\n",
        "                    self.sought_number += foundPositions\n",
        "                print(f\"\\r>> {foundPositions} positions found <<\".center(console.width))\n",
        "            titles = soup.select(c.title) if repo == 'scholarshipdb' else soup.find_all(class_=c.title)\n",
        "            countries = soup.select(c.country) if repo == 'scholarshipdb' else soup.find_all(class_=c.country)\n",
        "            dates = soup.select(c.date) if repo == 'scholarshipdb' else soup.find_all(class_=c.date)\n",
        "            links = soup.select(c.link) if repo == 'scholarshipdb' else soup.find_all(class_=c.link)\n",
        "\n",
        "            for title, country, date, link in zip(titles, countries, dates, links):\n",
        "                found_country = country.text if repo == 'scholarshipdb' else country.get('title', '')\n",
        "                save_position = not self.desired_countries or any(dc in found_country.lower() for dc in self.desired_countries)\n",
        "                if save_position:\n",
        "                    self.countries.append(found_country)\n",
        "                    self.titles.append(title.text.strip())\n",
        "                    self.dates.append(date.text.replace('\\n', '').strip())\n",
        "                    self.links.append(c.baseURL + link['href'])\n",
        "                    self.found_number += 1\n",
        "            print(f\"\\rPage {page} has been fetched from {c.baseURL}! Found {len(titles)} positions, saved {self.found_number}.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error in {repo} page {page}: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def prepare(self):\n",
        "        for repo in self.repos:\n",
        "            print(f\"\\r{('::[ '+repo+' ]::').center(console.width, '=')}\")\n",
        "            tasks = [asyncio.create_task(self.__get_page__(repo, page)) for page in range(1, self.maxpage+1)]\n",
        "            await asyncio.gather(*tasks)\n",
        "        print(f\"Total positions found: {self.sought_number}\")\n",
        "        print(f\"Positions saved after filtering: {self.found_number}\")\n",
        "\n",
        "    @property\n",
        "    def positions(self):\n",
        "        if self.df is None:\n",
        "            if self.titles:\n",
        "                positions = {\n",
        "                    \"Country\": self.countries,\n",
        "                    \"Date\": self.dates,\n",
        "                    \"Title\": self.titles,\n",
        "                    \"Link\": self.links\n",
        "                }\n",
        "                self.df = pd.DataFrame.from_dict(positions)\n",
        "                self.df['timedelta'] = self.df[\"Date\"].apply(lambda x: re.sub('about|ago', '', x).strip())\n",
        "                name2days ={'minutes':'*1', 'hours':'*60', 'hour':'*60', 'days':'*1440', 'day':'*1440',\n",
        "                            'weeks':'*10080', 'week':'*10080', 'months':'*43200', 'month':'*43200'}\n",
        "                self.df.replace({'timedelta': name2days }, regex=True, inplace=True)\n",
        "                self.df['timedelta'] = self.df['timedelta'].apply(lambda x: eval(x) if x else x)\n",
        "                self.df.sort_values(by=['Country', 'timedelta', 'Title'], inplace=True)\n",
        "                self.df = self.df.drop('timedelta', axis=1).reset_index(drop=True)\n",
        "            else:\n",
        "                print(\"No data was scraped. Creating an empty DataFrame.\")\n",
        "                self.df = pd.DataFrame(columns=[\"Country\", \"Date\", \"Title\", \"Link\"])\n",
        "        return self.df\n",
        "\n",
        "    def save(self, output='both'):\n",
        "        df = self.positions\n",
        "        if len(df) > 0:\n",
        "            s  = 's' if output=='both' else ''\n",
        "            print(f\"\\r{console.width*' '}\\n>>>> {self.sought_number} positions have been found in total.\",\n",
        "            f\"Specifically, {len(df)} records of them have been saved in the following file{s}:\" , sep='\\n')\n",
        "            if output in ('csv', 'both'):\n",
        "                df.to_csv(f'{self.file_name}.csv', index=False)\n",
        "                rich.print(f'[blue]{self.file_name}.csv saved![/blue]')\n",
        "            if output in ('xlsx', 'both'):\n",
        "                df.to_excel(f'{self.file_name}.xlsx', index=False)\n",
        "                rich.print(f'[blue]{self.file_name}.xlsx saved![/blue]')\n",
        "        else:\n",
        "            rich.print('[red blink] >>> No positions found, change your keyword or countries. <<< [/red blink]')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaINhsWcWrL9"
      },
      "source": [
        "## Step 5: Main Function & Export Links as JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J9OXkiRWrjm"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def main(keywords, maxpage=10, countries=None, output='both'):\n",
        "    print(f\"Searching for keywords: {keywords}\")\n",
        "    print(f\"Desired countries: {countries}\")\n",
        "    ps = PhDSeeker(keywords, maxpage=maxpage, desired_countries=countries)\n",
        "    await ps.prepare()\n",
        "    print(f\"Titles found: {len(ps.titles)}\")\n",
        "    print(f\"Countries found: {len(ps.countries)}\")\n",
        "    print(f\"Dates found: {len(ps.dates)}\")\n",
        "    print(f\"Links found: {len(ps.links)}\")\n",
        "    ps.save(output)\n",
        "    return ps.positions\n",
        "\n",
        "# Example usage\n",
        "keywords = 'Computer Science'\n",
        "maxpage = 2\n",
        "countries = 'United States, Belgium, Netherlands, Germany, Switzerland'\n",
        "output = 'both'\n",
        "\n",
        "df = asyncio.get_event_loop().run_until_complete(main(keywords, maxpage, countries, output))\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"DataFrame columns: {df.columns}\")\n",
        "print(f\"First few rows:\")\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwRqxfeNuhRL"
      },
      "outputs": [],
      "source": [
        "links = df['Link'].tolist()\n",
        "\n",
        "links_dict = {\"links\": links}\n",
        "\n",
        "with open('PhD_Programs_Links.json', 'w') as f:\n",
        "    json.dump(links_dict, f, indent=2)\n",
        "\n",
        "print(\"The links have been saved to PhD_Programs_Links.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6d8SxPY1Kr"
      },
      "source": [
        "## Step 6: Setup the AutoGen Environment and define utility functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tlaL-8rYtx6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Union\n",
        "import autogen\n",
        "\n",
        "# Utility functions\n",
        "def count_links(json_file: str) -> Dict[str, Union[int, List[str]]]:\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return {\"count\": len(data['links']), \"links\": data['links']}\n",
        "\n",
        "\n",
        "def fetch_job_description(job_link: str) -> str:\n",
        "    response = requests.get(job_link)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        job_description = soup.get_text()\n",
        "        return job_description\n",
        "    else:\n",
        "        raise ValueError(f\"Failed to fetch the job description. HTTP status code: {response.status_code}\")\n",
        "\n",
        "def extract_info(job_description: str) -> Dict[str, str]:\n",
        "    # This function would use GPT to extract the required information\n",
        "    # For now, we'll return a placeholder\n",
        "    return {\n",
        "        \"University Name\": \"\",\n",
        "        \"Program Name\": \"\",\n",
        "        \"International Students' Deadline for Applying\": \"\",\n",
        "        \"Start Date\": \"\",\n",
        "        \"Country\": \"\",\n",
        "        \"Name of Supervisor\": \"\",\n",
        "        \"Funding\": \"\",\n",
        "        \"Link to Program's Page\": \"\",\n",
        "        \"Short Description of Project\": \"\"\n",
        "    }\n",
        "\n",
        "def write_to_csv(data: List[Dict[str, str]], filename: str) -> None:\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7GyOGOeY_fp"
      },
      "source": [
        "## Step 7: Define Agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llNeGhiZZCnu"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "api_key = os.getenv('API_KEY')\n",
        "import autogen\n",
        "\n",
        "# Access the API key\n",
        "api_key = os.getenv('API_KEY')\n",
        "\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"api_key\": api_key\n",
        "    }\n",
        "]\n",
        "llm_config = {\n",
        "    \"cache_seed\": 42,\n",
        "    \"temperature\": 0,\n",
        "    \"config_list\": config_list,\n",
        "    \"timeout\": 120,\n",
        "}\n",
        "\n",
        "def termination_msg(x):\n",
        "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
        "\n",
        "# User Proxy Agent\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"UserProxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    is_termination_msg=termination_msg,\n",
        "    code_execution_config={\"work_dir\": \"phd_scraper\"},\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "# Link Counter Agent\n",
        "link_counter = autogen.AssistantAgent(\n",
        "    name=\"LinkCounter\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"You are responsible for counting and retrieving the links to be processed. After counting, you should pass the links to the WebScraper for processing.\"\n",
        ")\n",
        "\n",
        "web_scraper = autogen.AssistantAgent(\n",
        "    name=\"WebScraper\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"You are responsible for visiting each link, extracting the required information, and passing the compiled information to the CSVWriter.\"\n",
        ")\n",
        "\n",
        "\n",
        "# CSV Writer Agent\n",
        "csv_writer = autogen.AssistantAgent(\n",
        "    name=\"CSVWriter\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"You are responsible for writing the extracted information to a CSV file.\"\n",
        ")\n",
        "\n",
        "# Register functions\n",
        "autogen.register_function(\n",
        "    count_links,\n",
        "    caller=link_counter,\n",
        "    executor=user_proxy,\n",
        "    name=\"count_links\",\n",
        "    description=\"Count the number of links in a JSON file.\",\n",
        ")\n",
        "\n",
        "autogen.register_function(\n",
        "    fetch_job_description,\n",
        "    caller=web_scraper,\n",
        "    executor=user_proxy,\n",
        "    name=\"fetch_job_description\",\n",
        "    description=\"Fetch job description from a given URL.\",\n",
        ")\n",
        "\n",
        "autogen.register_function(\n",
        "    extract_info,\n",
        "    caller=web_scraper,\n",
        "    executor=user_proxy,\n",
        "    name=\"extract_info\",\n",
        "    description=\"Extract PhD program information from job description.\",\n",
        ")\n",
        "\n",
        "autogen.register_function(\n",
        "    write_to_csv,\n",
        "    caller=csv_writer,\n",
        "    executor=user_proxy,\n",
        "    name=\"write_to_csv\",\n",
        "    description=\"Write extracted information to a CSV file.\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFp8yrsQZFuI"
      },
      "source": [
        "## Step 8: Initiate group chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGGdgh2dZIW1"
      },
      "outputs": [],
      "source": [
        "group_chat = autogen.GroupChat(\n",
        "    agents=[user_proxy, link_counter, web_scraper, csv_writer],\n",
        "    messages=[],\n",
        "    speaker_selection_method=\"auto\",\n",
        "    max_round=15\n",
        ")\n",
        "\n",
        "manager = autogen.GroupChatManager(groupchat=group_chat, llm_config=llm_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkQhmuq_ZKks"
      },
      "source": [
        "## Step 9: Initiate the process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHPS-518ZM1I"
      },
      "outputs": [],
      "source": [
        "def process_phd_links(json_file: str) -> None:\n",
        "    initial_message = f\"\"\"\n",
        "    Please process the PhD program links in the file: {json_file}\n",
        "    1. Use the count_links function to get the total number of links and the actual links\n",
        "    2. For each link returned by count_links:\n",
        "       a. Use the fetch_job_description function to get the job description\n",
        "       b. Use the extract_info function to extract the required information\n",
        "    3. After processing all links, use the write_to_csv function to save the information\n",
        "    Once all links are processed, reply with 'TERMINATE'.\n",
        "    \"\"\"\n",
        "    user_proxy.initiate_chat(manager, message=initial_message)\n",
        "\n",
        "# Run the process\n",
        "process_phd_links(\"PhD_Programs_Links.json\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
